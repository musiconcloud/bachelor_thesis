{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcfa264-e9be-431d-a165-b4803b7b0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/andremsouza/python-som\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Union, Callable, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.preprocessing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.metrics import ndcg_score, dcg_score\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "from time import time\n",
    "from itertools import product\n",
    "from munkres import Munkres\n",
    "import math\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7da86c-652e-4948-953b-00bf377f58e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading dataset \n",
    "\n",
    "query_scores = np.fromfile(\"native-queries/native-query-scores.bin\", dtype='f')\n",
    "query_scores = query_scores.reshape(327, 20000)\n",
    "\n",
    "frame_ids = np.fromfile(\"native-queries/native-query-frame-IDs.bin\", dtype='i')\n",
    "frame_ids = frame_ids.reshape(327, 20000)\n",
    " \n",
    "frame_features = np.fromfile(\"native-queries/frame-features.bin\", dtype='f')\n",
    "frame_features = frame_features.reshape(20000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa049286-de91-4180-8f45-e8f604d94c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _asymptotic_decay(x: float, t: int, max_t: int) -> float:\n",
    "    \"\"\"\n",
    "    Asymptotic decay function. Can be used for both the learning_rate or the neighborhood_radius.\n",
    "    :param x: float: Initial x parameter\n",
    "    :param t: int: Current iteration\n",
    "    :param max_t: int: Maximum number of iterations\n",
    "    :return: float: Current state of x after t iterations\n",
    "    \"\"\"\n",
    "    return x / (1 + t / (max_t / 2))\n",
    "\n",
    "def _linear_decay(x: float, t: int, max_t: int) -> float:\n",
    "    \"\"\"\n",
    "    Linear decay function. Can be used for both the learning_rate or the neighborhood_radius.\n",
    "    :param x: float: Initial x parameter\n",
    "    :param t: int: Current iteration\n",
    "    :param max_t: int: Maximum number of iterations\n",
    "    :return: float: Current state of x after t iterations\n",
    "    \"\"\"\n",
    "    return x * (1.0 - t / max_t)\n",
    "\n",
    "def _euclidean_distance(a: Union[float, np.ndarray], b: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    return np.linalg.norm(np.subtract(a, b), ord=2, axis=-1)\n",
    "\n",
    "# rank distance functions\n",
    "@np.vectorize\n",
    "def frac_max(a, b):\n",
    "    return np.absolute(np.subtract(a, b)) / max(a, b)\n",
    "\n",
    "@np.vectorize\n",
    "def frac_min(a, b):\n",
    "    return np.absolute(np.subtract(a, b)) / min(a, b)\n",
    "\n",
    "@np.vectorize\n",
    "def log_sqrt(a, b):\n",
    "    \n",
    "    if np.absolute(np.subtract(a, b)) == 0:\n",
    "        return np.log(1)\n",
    "    \n",
    "    return np.log(np.absolute(np.subtract(a, b)))\n",
    "\n",
    "class SOM:   \n",
    "    \"\"\"\n",
    "    Features:\n",
    "        - Stepwise and batch training\n",
    "        - Random weight initialization\n",
    "        - Random sampling weight initialization\n",
    "        - Linear weight initialization (with PCA)\n",
    "        - Automatic selection of map size ratio (with PCA)\n",
    "        - Gaussian and Bubble neighborhood functions\n",
    "        - Support for custom decay functions\n",
    "        - Support for visualization (U-matrix, activation matrix)\n",
    "        - Support for supervised learning (label map)\n",
    "        - Support for NumPy arrays, Pandas DataFrames and regular lists of values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            x: Union[int, None],\n",
    "            y: Union[int, None],\n",
    "            input_len: int,\n",
    "            learning_rate: float = 0.5,\n",
    "            learning_rate_decay: Callable[[float, int, int], float] = _asymptotic_decay,\n",
    "            neighborhood_radius: float = 1.0,\n",
    "            neighborhood_radius_decay: Callable[[float, int, int], float] = _asymptotic_decay,\n",
    "            neighborhood_function: str = 'gaussian',\n",
    "            distance_function: str = 'euclidean',\n",
    "            random_seed: Union[int, None] = None,\n",
    "            data: Union[np.ndarray, pd.DataFrame, list, None] = None,\n",
    "        \n",
    "            # variables added for our purposes\n",
    "            var_adjustment=False, \n",
    "            var_param=0, \n",
    "            num_images=20000, \n",
    "            query_scores=np.zeros(shape=(20000,)), \n",
    "            frame_ids=np.zeros(shape=(20000,)),\n",
    "            rank_distance: str = ''\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the self-organizing map class.\n",
    "        :param x: int or NoneType: X dimension of the self-organizing map\n",
    "        :param y: int or NoneType: Y dimension of the self-organizing map\n",
    "        :param input_len: int: Number of features of the training dataset, i.e.,\n",
    "            number of elements of each node of the network.\n",
    "        :param learning_rate: float: Initial learning rate for the training process. Defaults to 0.5.\n",
    "            Note: The value of the learning_rate is irrelevant for the 'batch' training mode.\n",
    "        :param learning_rate_decay: function: Decay function for the learning_rate variable.\n",
    "            May be a predefined one from this package, or a custom function, with the same parameters and return type.\n",
    "            Defaults to _asymptotic_decay.\n",
    "        :param neighborhood_radius: float: Initial neighborhood radius for the training process. Defaults to 1.\n",
    "        :param neighborhood_radius_decay: function: Decay function for the neighborhood_radius variable.\n",
    "            May be a predefined one from this package, or a custom function, with the same parameters and return type.\n",
    "            Defaults to _asymptotic_decay\n",
    "        :param neighborhood_function: str: Neighborhood function name for the training process.\n",
    "            May be either 'gaussian' or 'bubble'.\n",
    "        :param distance_function: function: Function for calculating distances/dissimilarities between models of the\n",
    "            network.\n",
    "            May be a predefined one from this package, or a custom function, with the same parameters and return type.\n",
    "            Defaults to _euclidean_distance.\n",
    "        :param random_seed: int or None: Seed for NumPy random value generator. Defaults to None.\n",
    "        :param data: array-like: dataset for performing PCA.\n",
    "            Required when either x or y is None, for determining map size.\n",
    "        \n",
    "        ##### \n",
    "        :param rank_distance: string: used to specify distance function for calculating the distance between rank(node) and rank(image).\n",
    "            May be euclidean, fractional_dif_max, fractional_dif_min, or log_sqrt.\n",
    "            Defaults to '' \n",
    "        \"\"\"\n",
    "        \n",
    "        if (x, y) == (None, None):\n",
    "            raise ValueError('At least one of the dimensions (x, y) must be specified')\n",
    "        if x == None or y == None:\n",
    "            # If a dataset was given through **kwargs, select missing dimension with PCA\n",
    "            # The ratio of the (x, y) sizes will comply roughly with the ratio of the two largest principal components\n",
    "            if 'data' == None:\n",
    "                raise ValueError(\n",
    "                    \"If one of the dimensions is not specified, a dataset must be provided for automatic size \"\n",
    "                    \"initialization.\")\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "\n",
    "        # Update missing size variable\n",
    "        if x == None:\n",
    "            x = y // ratio\n",
    "        if y == None:\n",
    "            y = x // ratio\n",
    "            \n",
    "        # Initializing private variables\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self._shape = (np.uint(x), np.uint(y))\n",
    "        self._input_len = np.uint(input_len)\n",
    "        self._learning_rate = float(learning_rate)\n",
    "        self._learning_rate_decay = learning_rate_decay\n",
    "        self._neighborhood_radius = float(neighborhood_radius)\n",
    "        self._neighborhood_radius_decay = neighborhood_radius_decay\n",
    "        self._neighborhood_function = {\n",
    "            'gaussian': self._gaussian}[neighborhood_function]\n",
    "        \n",
    "        # string specifying distance function\n",
    "        self.distance_function = distance_function \n",
    "        # distance function itself\n",
    "        self._distance_function = {\n",
    "            'euclidean': _euclidean_distance,\n",
    "            'custom': self.custom_distance\n",
    "        }[distance_function]\n",
    "        \n",
    "        if distance_function == 'custom' and rank_distance == '':\n",
    "            raise ValueError(\"ERROR: rank distance function has not been selected despite custom distance function having been selected\") \n",
    "        if distance_function == 'custom' and rank_distance not in ['euclidean', 'fractional_dif_max', 'fractional_dif_min', 'log_sqrt']:\n",
    "            raise ValueError(\"ERROR: unavailable rnak distance function selected\") \n",
    "        else: \n",
    "            self.rank_distance = rank_distance\n",
    "            self.node_ranks = np.arange(1, self.x * self.y + 1).reshape(self.x, self.y) # matrix of SOM node ranks\n",
    "        \n",
    "        self._neigx, self._neigy = np.arange(self._shape[0]), np.arange(self._shape[1])\n",
    "        \n",
    "        self.data = data\n",
    "        self.query_scores = query_scores\n",
    "        self.num_images = num_images\n",
    "        \n",
    "        # empty dataframe with column names\n",
    "        self.selected_images = pd.DataFrame(columns = ['filename', 'ID', 'BMU_x', 'BMU_y'])\n",
    "        self.selected_vectors = np.zeros((self.x, self.y, 128))\n",
    "        \n",
    "        # Seed numpy random generator\n",
    "        if random_seed == None:\n",
    "            self._random_seed = np.random.randint(np.random.randint(np.iinfo(np.int32).max))\n",
    "        else:\n",
    "            self._random_seed = int(random_seed)\n",
    "        np.random.seed(self._random_seed)\n",
    "\n",
    "        ## using top N images only! ##\n",
    "        if num_images != 20000:\n",
    "            \n",
    "            ids = pd.DataFrame(frame_ids, columns=['ID'])\n",
    "            top_ids = ids[0:num_images]\n",
    "            self.query_scores = query_scores[0:num_images]\n",
    "            # fetch only top N images from dataset\n",
    "            self.data = data[top_ids.ID]\n",
    "        \n",
    "        # Random weight initialization\n",
    "        self._weights = np.random.standard_normal(size=(self._shape[0], self._shape[1], self._input_len))\n",
    "        \n",
    "        # adding extra dimension to the data\n",
    "        if input_len == 129: \n",
    "             \n",
    "            # adding query score column to data as 129th column\n",
    "            self.data = np.column_stack((self.data, self.query_scores))\n",
    "            # dividing new column by standard deviation to adjust its variance to 1\n",
    "            self.data[:,128] /= math.sqrt(self.data[:,128].var()) \n",
    "            \n",
    "            # adding query scores to SOM nodes' 129th dimension as well, considering the position of the nodes\n",
    "            k = 0\n",
    "            for i in range(self.x):\n",
    "                for j in range(self.y):\n",
    "                    self._weights[i, j, 128] = self.data[k, 128]\n",
    "                    k += 1\n",
    "\n",
    "    # remove the extra dimension added to the data \n",
    "    def reset_data(self):\n",
    "\n",
    "        self.data = np.delete(self.data, -1, axis=1)\n",
    "    \n",
    "    ### custom distance function for adding bias based on ranks of node and image ###\n",
    "    def custom_distance(self, data_row, weights):\n",
    "        # custom distance L_new(node, image) = L2(node, image) + X where\n",
    "        # X = 1/i * dist(rank_node, rank_image)\n",
    "        # where dist = Euclidean, Fractional difference, or log(sqrt(rank_node - rank_image))\n",
    "        \n",
    "        # initial distances between data and each weight vector \n",
    "        l2 = np.linalg.norm(np.subtract(data_row, weights), ord=2, axis=-1)\n",
    "\n",
    "        # given an input vector x, compare its 129th dim with dataset (X) and find its rank. \n",
    "        # to find the rank of the current input x, simply calculate the index of the current row vector\n",
    "        # because query scores are sorted in 129th dim of X.\n",
    "        image_rank = np.where((self.data==data_row).all(axis=1))[0][0] + 1 # adding 1 because index starts at 0 and we don't want to divide by 0\n",
    "\n",
    "        ##########\n",
    "        if self.rank_distance == 'euclidean':\n",
    "            dist = np.absolute(np.subtract(image_rank, self.node_ranks)) # euclidean\n",
    "        if self.rank_distance == 'fractional_dif_max':\n",
    "            dist = frac_max(image_rank, self.node_ranks)\n",
    "        if self.rank_distance == 'fractional_dif_min':\n",
    "            dist = frac_min(image_rank, self.node_ranks)\n",
    "        if self.rank_distance == 'log_sqrt':\n",
    "            dist = log_sqrt(image_rank, self.node_ranks)\n",
    "\n",
    "        node_weight = np.arange(1, self.x * self.y + 1).reshape(self.x, self.y)\n",
    "        # commenting this out for now\n",
    "        # node_weight = np.reciprocal(node_weight, dtype=np.float64) \n",
    "\n",
    "        result = l2 + node_weight * dist\n",
    "\n",
    "        return result;\n",
    "        \n",
    "    def activate(self, x: Union[np.ndarray, pd.DataFrame, list]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates distances between an instance x and the weights of the network.\n",
    "        :param x: array-like: Instance to be compared with the weights of the network.\n",
    "        :return: np.ndarray: Distances between x and each weight of the network.\n",
    "        \"\"\"\n",
    "        return self._distance_function(x, self._weights)\n",
    "\n",
    "    def winner(self, x: Union[np.ndarray, pd.DataFrame, list]) -> Union[Iterable, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Calculates the best-matching unit of the network for an instance x\n",
    "        :param x: array-like: Instance to be compared with the weights of the network.\n",
    "        :return: (int, int): Index of the best-matching unit of x.\n",
    "        \"\"\"\n",
    "        activation_map = self.activate(x)\n",
    "        return np.unravel_index(activation_map.argmin(), activation_map.shape)\n",
    "\n",
    "    def quantization(self, data: Union[np.ndarray, pd.DataFrame, list]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates distances from each instance of 'data' to each of the weights of the network.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: np.ndarray: array of lists of distances from each instance of the dataset\n",
    "            to each weight of the network.\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "        # Quantization - we wanna perform with Euclidean distance regardless of the distance function selected!\n",
    "        return np.array([(_euclidean_distance(i, self._weights[self.winner(i)])) for i in data_array])\n",
    "\n",
    "    def quantization_error(self, data: Union[np.ndarray, pd.DataFrame, list]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates average distance of the weights of the network to their assigned instances from data.\n",
    "        This error is a quality measure for the training process.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: float: Quantization error.\n",
    "        \"\"\"\n",
    "        quantization = self.quantization(data)\n",
    "        return quantization.mean()\n",
    "\n",
    "    def activation_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the activation matrix of the network for a dataset, i.e., for each node, the count of instances that\n",
    "        have been assigned to it, in the current state.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: np.ndarray: Activation matrix.\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(self.data, pd.DataFrame):\n",
    "            data_array = self.data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(self.data)\n",
    "\n",
    "        activation_matrix = np.zeros(self._shape)\n",
    "        for i in data_array:\n",
    "            activation_matrix[self.winner(i)] += 1\n",
    "        return activation_matrix\n",
    "    \n",
    "    def winner_map(self, data: Union[np.ndarray, pd.DataFrame, list]) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates, for each node (i, j) of the network,\n",
    "        the list of all instances from 'data' that has been assigned to it.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: dict: Winner map.\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "        winner_map = {(i, j): [] for i in range(self._shape[0]) for j in range(self._shape[1])}\n",
    "        for i in data_array:\n",
    "            winner_map[self.winner(i)].append(i)\n",
    "        return winner_map\n",
    "\n",
    "    def train(self, n_iteration: Union[int, None] = None,\n",
    "              mode: str = 'sequential', verbose: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Trains the self-organizing map, with the dataset 'data', and a certain number of iterations.\n",
    "        :param data: array-like: Dataset for training.\n",
    "        :param n_iteration: int or None: Number of iterations of training.\n",
    "            If None, defaults to 1000 * len(data) for stepwise training modes,\n",
    "            or 10 * len(data) for batch training mode.\n",
    "        :param mode: str: Training mode name. May be either 'random', 'sequential', or 'batch'.\n",
    "            For 'batch' mode, a much smaller number of iterations is needed, but a higher computation power is required\n",
    "            for each individual iteration.\n",
    "        :param verbose: bool: Activate to print useful information to the terminal/console, e.g.,\n",
    "            the progress of the training process\n",
    "        :return: float: Quantization error after training\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array for training\n",
    "        if isinstance(self.data, pd.DataFrame):\n",
    "            data_array = self.data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(self.data)\n",
    "\n",
    "        # If no number of iterations is given, select automatically\n",
    "        if n_iteration == None:\n",
    "            n_iteration = {'random': 1000, 'sequential': 1000, 'batch': 10}[mode] * len(data_array)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training with\", n_iteration,\n",
    "                  \"iterations.\\nTraining mode:\", mode, sep=' ')\n",
    "\n",
    "        if mode == 'sequential':\n",
    "            \n",
    "            # shuffling indexes\n",
    "            np.random.seed(self._random_seed)\n",
    "            indexes = np.arange(self.num_images)\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "            # Sequential sampling from training dataset\n",
    "            i = 0\n",
    "            while i < n_iteration:\n",
    "                \n",
    "                if self._input_len == 129 and i < n_iteration / 2:\n",
    "                    \n",
    "                    # rebiasing before each epoch - force-assign top scores to weights from top row, and vice versa\n",
    "                    sorted_data = np.sort(test.data[:,128])[::-1]\n",
    "                    # top row\n",
    "                    for j in range(self.y):\n",
    "                        self._weights[0, j, 128] = sorted_data[j]\n",
    "                    for k in range(self.y):\n",
    "                        self._weights[self.x - 1, k, 128] = sorted_data[(self.x-1) * self.y + k]\n",
    "                        \n",
    "                for idx in indexes:\n",
    "                    x = data_array[idx]\n",
    "\n",
    "                    # Calculating decaying alpha and sigma parameters for updating weights\n",
    "                    alpha = self._learning_rate_decay(self._learning_rate, i, n_iteration)\n",
    "                    sigma = self._neighborhood_radius_decay(\n",
    "                        self._neighborhood_radius, i, n_iteration)\n",
    "\n",
    "                    # Finding winner node (best-matching unit)\n",
    "                    winner = self.winner(x)\n",
    "\n",
    "                    # Updating weights, based on current neighborhood function\n",
    "                    self._weights += alpha * self._neighborhood_function(winner, sigma)[..., None] * (\n",
    "                            x - self._weights)\n",
    "\n",
    "                    # Print progress, if verbose is activated\n",
    "                    if verbose:\n",
    "                        print(\"Iteration:\", i, \"/\", n_iteration, sep=' ', end='\\r', flush=True)\n",
    "\n",
    "                    i += 1\n",
    "\n",
    "            print(\"final alpha: \", alpha)\n",
    "            print(\"final sigma: \", sigma)\n",
    "  \n",
    "        elif mode == 'batch':\n",
    "            # Batch training\n",
    "            for it in range(n_iteration):\n",
    "\n",
    "                # Calculating decaying sigma\n",
    "                sigma = self._neighborhood_radius_decay(\n",
    "                    self._neighborhood_radius, it, n_iteration)\n",
    "\n",
    "                # For each node, create a list of instances associated to it\n",
    "                winner_map = self.winner_map(data_array)\n",
    "\n",
    "                # Calculate the weighted average of all instances in the neighborhood of each node\n",
    "                new_weights = np.zeros(self._weights.shape)\n",
    "                for i in winner_map.keys():\n",
    "                    neig = self._neighborhood_function(i, sigma)\n",
    "                    upper, bottom = np.zeros(self._input_len), 0.0\n",
    "                    for j in winner_map.keys():\n",
    "                        upper += neig[j] * np.sum(winner_map[j], axis=0)\n",
    "                        bottom += neig[j] * len(winner_map[j])\n",
    "\n",
    "                    # Only update if there is any instance associated with the winner node or its neighbors\n",
    "                    if bottom != 0:\n",
    "                        new_weights[i] = upper / bottom\n",
    "                        \n",
    "                # Update all nodes concurrently\n",
    "                self._weights = new_weights\n",
    "\n",
    "                # Print progress, if verbose is activated\n",
    "                if verbose:\n",
    "                    print(\"Iteration:\", it, \"/\", n_iteration, sep=' ', end='\\r', flush=True)\n",
    "        else:\n",
    "            # Invalid training mode value\n",
    "            raise ValueError(\n",
    "                \"Invalid value for 'mode' parameter. Value should be in \" + str(['random', 'sequential', 'batch']))\n",
    "\n",
    "        # Compute quantization error\n",
    "        q_error = self.quantization_error(data_array)\n",
    "        if verbose:\n",
    "            print(\"Quantization error:\", q_error, sep=' ')\n",
    "        return q_error\n",
    "\n",
    "    def weight_initialization(self, mode: str = 'random',\n",
    "                              **kwargs: Union[np.ndarray, pd.DataFrame, list, str, int]) -> None:\n",
    "        \"\"\"\n",
    "        Function for weight initialization of the self-organizing map. Calls other methods for each initialization mode.\n",
    "        :param mode: str: Initialization mode. May be either 'random', 'linear', or 'sample'.\n",
    "            Note: Each initialization method may require multiple additional arguments in kwargs.\n",
    "        :param kwargs:\n",
    "            For 'random' initialization mode, 'sample_mode': str may be provided to determine the sampling mode.\n",
    "            'sample_mode' may be either 'standard_normal' (default) or 'uniform'.\n",
    "            For 'random' and 'sample' modes, 'random_seed': int may be provided for the random value generator.\n",
    "            For 'sample' and 'linear' modes, 'data': array-like must be provided for sampling/PCA.\n",
    "        \"\"\"\n",
    "        modes = {'random': self._weight_initialization_random,\n",
    "                 'sample': self._weight_initialization_sample}\n",
    "        try:\n",
    "            modes[mode](**kwargs)\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Invalid value for 'mode' parameter. Value should be in \" + str(modes.keys()))\n",
    "\n",
    "    def _weight_initialization_random(self, sample_mode: str = 'standard_normal',\n",
    "                                      random_seed: Union[int, None] = None) -> None:\n",
    "        \"\"\"\n",
    "        Random initialization method. Assigns weights from a random distribution defined by 'sample_mode'.\n",
    "        :param sample_mode: str: Distribution for random sampling. May be either 'uniform' or 'standard_normal'.\n",
    "            Defaults to 'standard_normal'.\n",
    "        :param random_seed: int or None: Seed for NumPy random value generator. Defaults to None.\n",
    "        \"\"\"\n",
    "        sample_modes = {'uniform': np.random.random, 'standard_normal': np.random.standard_normal}\n",
    "\n",
    "        # Seed numpy random generator\n",
    "        if random_seed == None:\n",
    "            random_seed = np.random.randint(\n",
    "                np.random.randint(np.iinfo(np.int32).max))\n",
    "        else:\n",
    "            random_seed = int(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        # Initialize weights randomly\n",
    "        try:\n",
    "            self._weights = sample_modes[sample_mode](size=self._weights.shape)\n",
    "        except KeyError:\n",
    "            raise ValueError(\n",
    "                \"Invalid value for 'sample_mode' parameter. Value should be in \" + str(sample_modes.keys()))\n",
    "\n",
    "    def _weight_initialization_sample(self, data: Union[np.ndarray, pd.DataFrame, list],\n",
    "                                      random_seed: Union[int, None] = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialization method. Assigns weights to random samples from an input dataset.\n",
    "        :param data: Dataset for weight initialization/sampling.\n",
    "        :param random_seed: int or None: Seed for NumPy random value generator. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Seed numpy random generator\n",
    "        if random_seed == None:\n",
    "            random_seed = np.random.randint(\n",
    "                np.random.randint(np.iinfo(np.int32).max))\n",
    "        else:\n",
    "            random_seed = int(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        # Convert data to numpy array for training\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "\n",
    "        # Assign weights to random samples from dataset\n",
    "        sample_size = self._shape[0] * self._shape[1]\n",
    "        sample = np.random.choice(len(data_array), size=sample_size,\n",
    "                                  replace=(sample_size > len(data_array)))\n",
    "        self._weights = data_array[sample].reshape(self._weights.shape)\n",
    "\n",
    "    def _gaussian(self, c: Tuple[int, int], sigma: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gaussian neighborhood function, centered in c. \n",
    "        :param c: (int, int): Center coordinates for gaussian function.\n",
    "        :param sigma: float: Spread variable for gaussian function.\n",
    "        :return: np.ndarray: Gaussian, centered in c, over all the weights of the network.\n",
    "        \"\"\"\n",
    "        # Calculate coefficient with sigma\n",
    "        d = 2 * sigma * sigma\n",
    "        # Calculate vertical and horizontal distances\n",
    "        dx = self._neigx - c[0]\n",
    "        dy = self._neigy - c[1]\n",
    "\n",
    "        # Calculate gaussian centered in c\n",
    "        ax = np.exp(-np.power(dx, 2) / d)\n",
    "        ay = np.exp(-np.power(dy, 2) / d)\n",
    "        return np.outer(ax, ay)\n",
    "    \n",
    "    \n",
    "    ## custom functions for displaying images ##\n",
    "    \n",
    "    ### for RGB example ###\n",
    "    def rgb(self):\n",
    "        \n",
    "        data_ = pd.DataFrame(columns=['BMU_x','BMU_y', 'R', 'G', 'B'])\n",
    "        i = 0\n",
    "        for vec in colors:\n",
    "            bmu_index = self.winner(vec)\n",
    "\n",
    "            my_dict = {'BMU_x': bmu_index[0], 'BMU_y': bmu_index[1], 'R': vec[0], 'G': vec[1], 'B': vec[2]} \n",
    "            data_ = data_.append(my_dict, ignore_index=True)\n",
    "            data_[['BMU_x', 'BMU_y']] = data_[['BMU_x', 'BMU_y']].astype(int)\n",
    "\n",
    "        selected_images = pd.DataFrame(columns = ['BMU_x', 'BMU_y', 'R', 'G', 'B'])\n",
    "\n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "\n",
    "                tmp = data_.loc[(data_.BMU_x == i) & (data_.BMU_y == j)]\n",
    "                if(len(tmp) != 0):\n",
    "\n",
    "                    ## random selection (for now) ##\n",
    "                    sample = tmp.sample()\n",
    "                    selected_images = selected_images.append(sample)\n",
    "\n",
    "        print(\"The following images were selected: \")            \n",
    "        display(selected_images)\n",
    "        \n",
    "        # visualizing selected images \n",
    "        fig, ax = plt.subplots(self.x, self.y, sharex='col', sharey='row', figsize=(4,4))\n",
    "        \n",
    "        for i in range(len(selected_images)):\n",
    "      \n",
    "            row = selected_images.iloc[i,:]\n",
    "        \n",
    "            rgb = np.asarray([row['R'], row['G'], row['B']])\n",
    "            \n",
    "            x = row.BMU_x\n",
    "            y = row.BMU_y\n",
    "            \n",
    "            ax[x, y].imshow([[rgb]])\n",
    "            \n",
    "            ax[x, y].set_xticklabels([])\n",
    "            ax[x, y].set_yticklabels([])\n",
    "        plt.grid(False)\n",
    "        plt.axis('off')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "    ############################\n",
    "    \n",
    "    \n",
    "    def display_results(self, frameID_scores, original_data, ranking=False):\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        # putting the 3 functions below together\n",
    "        filepaths = pd.read_csv(r\"native-queries\\frame-ID-to-filepath.csv\",sep=' ',names=['filename', 'ID'])\n",
    "        \n",
    "        if self.num_images < 20000:\n",
    "            frameID_scores = frameID_scores.head(self.num_images)\n",
    "        \n",
    "        if self.num_images == (self.x * self.y):\n",
    "            self.hungarian_algo(filepaths, frameID_scores)\n",
    "        \n",
    "        else:\n",
    "            df = self.node2image(filepaths)\n",
    "            self.select_images(df, frameID_scores)\n",
    "        \n",
    "        self.display_images(original_data)\n",
    "        \n",
    "        if ranking == True:\n",
    "            self.add_ranking()\n",
    "\n",
    "        print(\"Time elapsed: %s seconds\" % (time() - start_time))\n",
    "    \n",
    "    def hungarian_algo(self, filepaths, frameID_scores):\n",
    "        \n",
    "        # frameID_scores = frameID_scores[0:self.num_images]\n",
    "        # self.selected_images = pd.DataFrame(columns = ['filename', 'ID', 'BMU_x', 'BMU_y'])\n",
    "        \n",
    "        # merge with filepaths on top 100 IDs only \n",
    "        merged = frameID_scores.merge(filepaths, how='inner', on='ID') \n",
    "    \n",
    "        # pairwise distance matrix for SOM weight vectors and input data \n",
    "        pairwise_dist = distance_matrix(self.data, np.concatenate(self._weights)) #swapped back to normal again\n",
    "        m = Munkres()\n",
    "        indexes = m.compute(pairwise_dist)\n",
    "\n",
    "        my_pd = pd.DataFrame(columns = ['ID', 'BMU_x', 'BMU_y'])\n",
    "\n",
    "        for row, column in indexes:\n",
    "\n",
    "            x = row / 10\n",
    "            y = row % 10\n",
    "            my_pd = my_pd.append({'ID': merged.ID[column],'BMU_x': int(x), 'BMU_y': y}, ignore_index=True)\n",
    "\n",
    "        merged = my_pd.merge(merged, on='ID')\n",
    "        \n",
    "        print(\"The following images were selected by Hungarian algo: \")\n",
    "        display(merged)\n",
    "        \n",
    "        self.selected_images = merged\n",
    "    \n",
    "    def node2image(self, filepaths):\n",
    "\n",
    "        # run find_bmu to obtain BMU for each feature vector\n",
    "        bmu_list = []\n",
    "        for vec in self.data:\n",
    "            bmu_list.append(np.asarray(self.winner(vec)))\n",
    "\n",
    "        # storing the result in a dataframe. \n",
    "        # i-th row of the dataframe (ex: 4 1) is associated with the BMU coordinates for the i-th feature vector  \n",
    "        data_ = pd.DataFrame(bmu_list, columns=['BMU_x','BMU_y'])\n",
    "\n",
    "        # merge the 2 dataframes\n",
    "        df = pd.concat([filepaths, data_], axis=1, join='inner')\n",
    "\n",
    "        # now we have:\n",
    "\n",
    "        # filename ID BMU_x BMU_y\n",
    "        # ........ rows ........\n",
    "\n",
    "        return df\n",
    "\n",
    "    # image selection after training\n",
    "    def select_images(self, df, id_scores):\n",
    "\n",
    "        # empty dataframe with column names\n",
    "        ##selected_images = self.selected_images\n",
    "        selected_images = pd.DataFrame(columns = ['filename', 'ID', 'BMU_x', 'BMU_y'])\n",
    "        \n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "\n",
    "                tmp = df.loc[(df.BMU_x == i) & (df.BMU_y == j)]\n",
    "                display(tmp)\n",
    "                if(len(tmp) != 0):\n",
    "                    \n",
    "                    merged = tmp.merge(id_scores, how='inner', on='ID') \n",
    "                    \n",
    "                    # select image with highest score\n",
    "                    highest = merged.loc[merged['query_scores'].idxmax()]\n",
    "                    \n",
    "                    selected_images = selected_images.append(highest)\n",
    "\n",
    "        print(\"The following images were selected: \")\n",
    "        display(selected_images)\n",
    "\n",
    "        self.selected_images = selected_images\n",
    "    \n",
    "    def display_images(self, original_data):\n",
    "\n",
    "        # this function not only displays images, but also removes the extra dimension we added to the data\n",
    "        # and extracts selected vectors for each SOM node\n",
    "        \n",
    "        my_dir = \"native-queries/thumbs/\"\n",
    "       \n",
    "        fig, ax = plt.subplots(self.x, self.y, sharex='col', sharey='row', figsize=(32 * self.y, 18  * self.x))\n",
    "        \n",
    "        # remove the extra dimension we initially added to the data\n",
    "        if self._input_len == 129:\n",
    "            self.reset_data()\n",
    "        for i in range(len(self.selected_images)):\n",
    "\n",
    "            row = self.selected_images.iloc[i,:]\n",
    "            x = row.BMU_x\n",
    "            y = row.BMU_y\n",
    "            id = row.ID\n",
    "        \n",
    "            # extracting selected vectors used for output - for evaluation purposes later\n",
    "            self.selected_vectors[x, y] = original_data[id]\n",
    "\n",
    "            img = Image.open(my_dir + row.filename)\n",
    "            img = img.resize((320, 180))\n",
    "            img = np.asarray(img)\n",
    "\n",
    "            ax[x, y].imshow(img, aspect='auto')\n",
    "            \n",
    "            # add a patch to visualize the original ranking of the associated image\n",
    "            # top image from original ranking is colored mint, and others shades of green depending on their query scores\n",
    "            if row.query_scores == self.selected_images['query_scores'].max():\n",
    "                rect = patches.Rectangle((275, 150), 40, 25, color=(0.07, 0.95, 0.9), fill=True)\n",
    "                \n",
    "            else:\n",
    "                rect = patches.Rectangle((275, 150), 40, 25, color=(0, row.query_scores, 0), fill=True)\n",
    "            \n",
    "            ax[x, y].add_patch(rect)\n",
    "        \n",
    "        #plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)\n",
    "        \n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "        plt.axis('scaled')\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "   # adding ranking to selected images \n",
    "    def add_ranking(self):\n",
    "        \n",
    "        selected = self.selected_images\n",
    "        \n",
    "        selected['SOM_rank'] = np.arange(len(selected)) + 1\n",
    "        selected['QS_rank'] = selected['query_scores'].rank(ascending=False).astype(int)\n",
    "    \n",
    "        self.selected_images = selected\n",
    "        \n",
    "        print(\"ranking: \")\n",
    "        display(selected)\n",
    " \n",
    "    ## evaluation metrics ## \n",
    "    # comparing the distance of selected images on the output screen and in the feature space\n",
    "    \n",
    "    # here we use 2 different notions of \"neighbor\" - direct neighbors (neighbors in all cardinal directions) and next door neighbors.\n",
    "    def mean_distance(self, print_result=False):\n",
    "        # mean distance matrix[i,j] contains the mean distance of neighbors from selected_vectors[i, j]\n",
    "        mean_distance_matrix = np.zeros((som_mk.x, som_mk.y))\n",
    "        distance_sum = 0\n",
    "        \n",
    "        # hardcoding. Not elegent but well, it works. #\n",
    "        for i in range(som_mk.x):\n",
    "            for j in range(som_mk.y):\n",
    "                # 4 corners \n",
    "                distance_sum = 0\n",
    "\n",
    "                if i == 0 and j == 0:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[0, 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[1, 0])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[1, 1])\n",
    "                    distance_sum /= 3\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                if i == 0 and j == som_mk.y - 1:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[0, som_mk.y - 2])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[1, som_mk.y - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[1, som_mk.y - 2])\n",
    "                    distance_sum /= 3\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                if i == som_mk.x - 1 and j == 0: \n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[som_mk.x - 2, 0])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[som_mk.x - 2, 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[som_mk.x - 1, 1])\n",
    "                    distance_sum /= 3\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                if i == som_mk.x - 1 and j == som_mk.y - 1:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[som_mk.x - 2, som_mk.y - 2])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[som_mk.x - 2, som_mk.y - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[som_mk.x - 1, som_mk.y - 2])\n",
    "                    distance_sum /= 3\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                # 4 sides\n",
    "                if i == 0 and j != som_mk.y - 1:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j + 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j + 1])\n",
    "                    distance_sum /= 5\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                elif i == som_mk.x - 1 and j != som_mk.y - 1:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j + 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j + 1])\n",
    "                    distance_sum /= 5\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                elif j == 0 and i != som_mk.x - 1:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j + 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j + 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j + 1])\n",
    "                    distance_sum /= 5\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                elif j == som_mk.y - 1 and i != som_mk.x - 1:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j])\n",
    "                    distance_sum /= 5\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                # other cases:\n",
    "                elif i != som_mk.x - 1 and j != som_mk.y - 1:\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i - 1, j + 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i, j + 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j - 1])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j])\n",
    "                    distance_sum += _euclidean_distance(som_mk.selected_vectors[i, j], som_mk.selected_vectors[i + 1, j + 1])\n",
    "                    distance_sum /= 8\n",
    "                    mean_distance_matrix[i, j] = distance_sum\n",
    "                    \n",
    "        if print_result == True:\n",
    "            print(\"\\nmean_distance: \\n\")\n",
    "            print(mean_distance_matrix)\n",
    "\n",
    "        return mean_distance_matrix\n",
    "                \n",
    "    def mean_distance_nextdoor(self, print_result=False):\n",
    "        \n",
    "        mean_distance_matrix = np.zeros((self.x, self.y))\n",
    "        \n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "                if j == 0:\n",
    "                    mean_distance_matrix[i, j] = _euclidean_distance(self.selected_vectors[i,j+1], self.selected_vectors[i, j])\n",
    "                elif j == self.y-1:\n",
    "                    mean_distance_matrix[i, j] = _euclidean_distance(self.selected_vectors[i, j-1], self.selected_vectors[i, j])\n",
    "                else:\n",
    "                    mean_distance_matrix[i, j] = (_euclidean_distance(self.selected_vectors[i, j-1], self.selected_vectors[i, j]) \n",
    "                    + _euclidean_distance(self.selected_vectors[i, j+1], self.selected_vectors[i, j])) / 2\n",
    "                    \n",
    "        if print_result == True:\n",
    "            print(\"\\nmean_distance_nextdoor: \\n\")\n",
    "            print(mean_distance_matrix)\n",
    "                    \n",
    "        return mean_distance_matrix\n",
    "    \n",
    "    ## Rank correlation metrics ## \n",
    "    # comparing the original ranking with SOM-induced ranking\n",
    "        \n",
    "    def tau(self):\n",
    "        \n",
    "        return self.selected_images['SOM_rank'].corr(self.selected_images['QS_rank'], method='kendall')\n",
    "\n",
    "    def nDCG(self):\n",
    "        ## use original scores, not rankings. ##\n",
    "    \n",
    "        # Releveance scores in actual order\n",
    "        actual = self.selected_images['query_scores']\n",
    "        # in ideal order\n",
    "        ideal = actual.sort_values()\n",
    "        \n",
    "        actual = np.asarray(actual).reshape(1, self.num_images)\n",
    "        ideal = np.asarray(ideal).reshape(1, self.num_images)\n",
    "\n",
    "        return ndcg_score(actual, ideal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee283c-94f2-45df-ad99-0b5685612841",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SOM(x=20, y=10, input_len=129, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[0], frame_ids=frame_ids[0], var_adjustment=True)\n",
    "\n",
    "test.train(mode='sequential', n_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1c01c-82df-415a-b7ed-92e66af2a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[0], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "test.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b1399-56e2-49d8-9ee9-a741ad4af127",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i=1..no_epochs:\n",
    "  # biasing step\n",
    "  weights[:,129] = sort(selection[:,129])\n",
    "  # training epoch step\n",
    "  som_epoch(train weights from selection)\n",
    "\"\"\"\n",
    "weights = np.random.standard_normal(size=(20, 10, 129))\n",
    "before = weights[:, :, 128]\n",
    "print(\"before: \", weights[:, :, 128])\n",
    "#print(\"before: \", before)\n",
    "\n",
    "#test._weights[:, :, 128] = \n",
    "\n",
    "sorted_data = np.sort(test.data[:,128])[::-1]\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for j in range(10):\n",
    "    weights[0, j, 128] = sorted_data[j]\n",
    "for k in range(10):\n",
    "    weights[19, k, 128] = sorted_data[19 * 10 + k]\n",
    "\n",
    "after = weights[:, :, 128]\n",
    "\n",
    "before == after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28de4f-63f2-4ee4-8339-2f20fb3362c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_som = SOM(x=20, y=10, input_len=128, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[0], frame_ids=frame_ids[0])\n",
    "\n",
    "vanilla_som.train(mode='sequential', n_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd328a12-906f-4829-9e7c-13447c9018a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_som.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb771a3-02b1-4863-a6e7-e610cd53f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank_distance = euclidean, fractional_dif_max, fractional_dif_min, log_sqrt\n",
    "som_LP = SOM(x=20, y=10, input_len=128, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[1], frame_ids=frame_ids[1],\n",
    "        distance_function='custom', rank_distance='euclidean')\n",
    "\n",
    "som_LP.train(mode='sequential', n_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197c2cd-a239-406f-8584-925e39832f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[1], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[1], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "som_LP.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579cf53-f530-4888-9e2a-18aa285e0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    display(som_LP.selected_images)\n",
    "som_LP.selected_images.to_csv('results/som_LP_log-NoReciprocal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d5058-308c-4d67-9b92-1800c57713ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regular SOM\n",
    "som = SOM(x=20, y=10, input_len=128, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[0], frame_ids=frame_ids[0])\n",
    "\n",
    "som.train(mode='sequential', n_iteration=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27867e-796d-4db1-b604-f907a8ce972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[0], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "som.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0975d96-8582-4682-8139-c969a237f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(som.selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70090f44-382c-436d-9d3d-1249e1b77c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk = SOM(x=20, y=10, input_len=129, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "             neighborhood_function='gaussian', data=frame_features, random_seed=1903, \n",
    "             auto_var_adjustment=True, num_images=200, query_scores=query_scores[0], frame_ids=frame_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e6041-fc5f-4bb5-8109-ac72308d2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk.train(mode='sequential', n_iteration=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2630d-0407-458a-9ee7-b4393c77f031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[0], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "som_mk.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbf9e2-c1ea-47da-bea5-0461a2be4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = som.mean_distance(print_result=True)\n",
    "d2 = som.mean_distance_nextdoor(print_result=True)\n",
    "\n",
    "print(\"\\n=============\\n\")\n",
    "print(d1.mean(axis=1))\n",
    "print(d2.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf95c8-4267-4a05-b791-bfa71ed0c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = som_mk.mean_distance(print_result=True)\n",
    "d4 = som_mk.mean_distance_nextdoor(print_result=True);\n",
    "\n",
    "print(\"\\n=============\\n\")\n",
    "print(d3.mean(axis=1))\n",
    "print(d4.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72519834-8660-40df-8c77-48c682c70c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "som.tau()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c33c3-fce0-4c41-b729-b53dedec892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "som.nDCG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225b257-23c6-4f82-b1c4-9f1daf16a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk.tau()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c957e-edaa-4430-86b1-31455e0b9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk.nDCG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c3311-4178-4bb4-91a4-6e798461d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating random colors ###\n",
    "from skimage import io \n",
    "\n",
    "#np.random.seed(1)\n",
    "tuple_size = (10000, 3)\n",
    "colors = np.random.choice(range(256), size=tuple_size)\n",
    "colors = colors / 256\n",
    "\n",
    "#indices = np.random.randint(0, len(colors), size=tuple_size)\n",
    "#io.imshow(colors)\n",
    "\n",
    "\n",
    "plt.imshow([[colors[0]]])\n",
    "\n",
    "print(colors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f556c709-ec5d-4347-a1dc-00b7e57e92d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_RGB = SOM(x=20, y=20, input_len=3, learning_rate=0.5, neighborhood_radius=1.0, neighborhood_function='gaussian', data=colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e79855-c4a2-4a90-a587-eee5b95fb820",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_RGB.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f21363-18d3-4eaf-bfd8-9a42ccd0bf38",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_RGB.rgb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de71d1a-5734-45e2-9298-5074952cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(som_mk.selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d57ce-c360-4765-835f-e4aab1c88c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 200\n",
    "\n",
    "####\n",
    "# fetching top N frames for the very first query\n",
    "ids = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "top_ids = ids[0:num_images]\n",
    "data = frame_features[top_ids.ID]\n",
    "\n",
    "qs = query_scores[0]\n",
    "top_qs = qs[0:num_images]\n",
    "data = np.column_stack((data, top_qs))\n",
    "\n",
    "# example input row\n",
    "x = data[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aacfa5-8b14-49da-8179-5901999b35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = row vector, b = weights\n",
    "node_ranks = np.arange(1, 20 * 10 + 1).reshape(20, 10) # 10 x 10 matrix of node ranks\n",
    "image_rank = np.where((data==x).all(axis=1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ad769-440a-4ff5-8e65-ed49c00273ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running SOM on all 327 queries\n",
    "\n",
    "mean_dist = np.zeros(shape=(10,10))\n",
    "mean_distN = np.zeros(shape=(10,10))\n",
    "tau = 0\n",
    "nDCG = 0\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "reps = 100\n",
    "\n",
    "for i in range(0, reps):\n",
    "    biased_som = SOM(10, 10, frame_features[0].shape[0], data=frame_features, sigma=8.0, learning_rate=0.5, random_seed=1903, \n",
    "               query_scores=query_scores[i], frame_ids=frame_ids[i], auto_var_adjustment=True, num_images=100)\n",
    "    \n",
    "    biased_som.train_batch(15)\n",
    "    \n",
    "    a = pd.DataFrame(frame_ids[i], columns=['ID'])\n",
    "    b = pd.DataFrame(query_scores[i], columns=['query_scores'])\n",
    "    ab = pd.concat([a, b], axis=1, join='inner')\n",
    "    \n",
    "    biased_som.display_results(ab, frame_features)\n",
    "    \n",
    "    mean_dist += biased_som.mean_distance()\n",
    "    mean_distN += biased_som.mean_distance_nextdoor()\n",
    "    \n",
    "    tau += biased_som.tau()\n",
    "    nDCG += biased_som.nDCG()\n",
    "    \n",
    "    print(\"i: {}\".format(i))\n",
    "\n",
    "mean_dist /= reps\n",
    "mean_distN /= reps\n",
    "\n",
    "print(\"mean distance between surrounding neighbors: \\n{}\".format(mean_dist))\n",
    "print(\"mean distance between next door neighbors: \\n{}\".format(mean_distN))\n",
    "\n",
    "tau /= reps\n",
    "nDCG /= reps\n",
    "\n",
    "print(\"Kendall's Tau Coefficient: {}\".format(tau))\n",
    "print(\"nDCG: {}\".format(nDCG))\n",
    "\n",
    "print(\"Time elapsed: %s seconds\" % (time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
