{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adcfa264-e9be-431d-a165-b4803b7b0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modified from https://github.com/andremsouza/python-som\n",
    "\n",
    "from collections import Counter\n",
    "from typing import Union, Callable, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True, precision=5)\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "import sklearn\n",
    "import sklearn.decomposition\n",
    "import sklearn.preprocessing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from skimage import io \n",
    "\n",
    "from sklearn.metrics import ndcg_score, dcg_score\n",
    "from scipy.spatial import distance_matrix\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import math\n",
    "import scipy.stats as stats\n",
    "\n",
    "from time import time\n",
    "from itertools import product\n",
    "from munkres import Munkres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7da86c-652e-4948-953b-00bf377f58e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# loading dataset \n",
    "\n",
    "query_scores = np.fromfile(\"native-queries/native-query-scores.bin\", dtype='f')\n",
    "query_scores = query_scores.reshape(327, 20000)\n",
    "\n",
    "frame_ids = np.fromfile(\"native-queries/native-query-frame-IDs.bin\", dtype='i')\n",
    "frame_ids = frame_ids.reshape(327, 20000)\n",
    " \n",
    "frame_features = np.fromfile(\"native-queries/frame-features.bin\", dtype='f')\n",
    "frame_features = frame_features.reshape(20000, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa049286-de91-4180-8f45-e8f604d94c12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _asymptotic_decay(x: float, t: int, max_t: int) -> float:\n",
    "    \"\"\"\n",
    "    Asymptotic decay function. Can be used for both the learning_rate or the neighborhood_radius.\n",
    "    :param x: float: Initial x parameter\n",
    "    :param t: int: Current iteration\n",
    "    :param max_t: int: Maximum number of iterations\n",
    "    :return: float: Current state of x after t iterations\n",
    "    \"\"\"\n",
    "    return x / (1 + t / (max_t / 2))\n",
    "\n",
    "def _linear_decay(x: float, t: int, max_t: int) -> float:\n",
    "    \"\"\"\n",
    "    Linear decay function. Can be used for both the learning_rate or the neighborhood_radius.\n",
    "    :param x: float: Initial x parameter\n",
    "    :param t: int: Current iteration\n",
    "    :param max_t: int: Maximum number of iterations\n",
    "    :return: float: Current state of x after t iterations\n",
    "    \"\"\"\n",
    "    return x * (1.0 - t / max_t)\n",
    "\n",
    "def _euclidean_distance(a: Union[float, np.ndarray], b: Union[float, np.ndarray]) -> Union[float, np.ndarray]:\n",
    "    return np.linalg.norm(np.subtract(a, b), ord=2, axis=-1)\n",
    "\n",
    "def _cosine_distance(x, weights):\n",
    "    num = (weights * x).sum(axis=2)\n",
    "    denum = np.multiply(np.linalg.norm(weights, axis=2), np.linalg.norm(x))\n",
    "    \n",
    "    return 1 - num / (denum+1e-8)\n",
    "\n",
    "def _manhattan_distance(x, weights):\n",
    "    return np.linalg.norm(np.subtract(x, weights), ord=1, axis=-1)\n",
    "\n",
    "def _chebyshev_distance(x, weights):\n",
    "    return np.max(np.subtract(x, weights), axis=-1)\n",
    "\n",
    "# rank distance functions\n",
    "@np.vectorize\n",
    "def frac_max(a, b):\n",
    "    return np.absolute(np.subtract(a, b)) / max(a, b)\n",
    "\n",
    "@np.vectorize\n",
    "def frac_min(a, b):\n",
    "    return np.absolute(np.subtract(a, b)) / min(a, b)\n",
    "\n",
    "@np.vectorize\n",
    "def log_sqrt(a, b):\n",
    "    \n",
    "    if np.absolute(np.subtract(a, b)) == 0:\n",
    "        return np.log(1)\n",
    "    \n",
    "    return np.log(np.absolute(np.subtract(a, b)))\n",
    "\n",
    "class SOM:   \n",
    "    \"\"\"\n",
    "    Features:\n",
    "        - Stepwise and batch training\n",
    "        - Random weight initialization\n",
    "        - Random sampling weight initialization\n",
    "        - Linear weight initialization (with PCA)\n",
    "        - Automatic selection of map size ratio (with PCA)\n",
    "        - Gaussian and Bubble neighborhood functions\n",
    "        - Support for custom decay functions\n",
    "        - Support for visualization (U-matrix, activation matrix)\n",
    "        - Support for supervised learning (label map)\n",
    "        - Support for NumPy arrays, Pandas DataFrames and regular lists of values\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self,\n",
    "            x: Union[int, None],\n",
    "            y: Union[int, None],\n",
    "            input_len: int,\n",
    "            learning_rate: float = 0.5,\n",
    "            learning_rate_decay: Callable[[float, int, int], float] = _asymptotic_decay,\n",
    "            neighborhood_radius: float = 1.0,\n",
    "            neighborhood_radius_decay: Callable[[float, int, int], float] = _asymptotic_decay,\n",
    "            neighborhood_function: str = 'gaussian',\n",
    "            distance_function: str = 'euclidean',\n",
    "            random_seed: Union[int, None] = None,\n",
    "            data: Union[np.ndarray, pd.DataFrame, list, None] = None,\n",
    "        \n",
    "            # variables added for our purposes\n",
    "            var_adjustment=False, \n",
    "            var_param=0, \n",
    "            num_images=20000, \n",
    "            query_scores=np.zeros(shape=(20000,)), \n",
    "            frame_ids=np.zeros(shape=(20000,)),\n",
    "            rank_distance: str = '',\n",
    "            mode: str = ''\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Constructor for the self-organizing map class.\n",
    "        :param x: int or NoneType: X dimension of the self-organizing map\n",
    "        :param y: int or NoneType: Y dimension of the self-organizing map\n",
    "        :param input_len: int: Number of features of the training dataset, i.e.,\n",
    "            number of elements of each node of the network.\n",
    "        :param learning_rate: float: Initial learning rate for the training process. Defaults to 0.5.\n",
    "            Note: The value of the learning_rate is irrelevant for the 'batch' training mode.\n",
    "        :param learning_rate_decay: function: Decay function for the learning_rate variable.\n",
    "            May be a predefined one from this package, or a custom function, with the same parameters and return type.\n",
    "            Defaults to _asymptotic_decay.\n",
    "        :param neighborhood_radius: float: Initial neighborhood radius for the training process. Defaults to 1.\n",
    "        :param neighborhood_radius_decay: function: Decay function for the neighborhood_radius variable.\n",
    "            May be a predefined one from this package, or a custom function, with the same parameters and return type.\n",
    "            Defaults to _asymptotic_decay\n",
    "        :param neighborhood_function: str: Neighborhood function name for the training process.\n",
    "            May be either 'gaussian' or 'bubble'.\n",
    "        :param distance_function: str: used to specify metric for distances/dissimilarities between models of the\n",
    "            network.\n",
    "            May be a predefined one from this package, or a custom function, with the same parameters and return type.\n",
    "            May be 'euclidean', 'custom', or 'cosine'.\n",
    "            Defaults to 'euclidean'\n",
    "        :param random_seed: int or None: Seed for NumPy random value generator. Defaults to None.\n",
    "        :param data: array-like: dataset for performing PCA.\n",
    "            Required when either x or y is None, for determining map size.\n",
    "        \n",
    "        ##### \n",
    "        :param rank_distance: string: used to specify distance function for calculating the distance between rank(node) and rank(image).\n",
    "            May be euclidean, fractional_dif_max, fractional_dif_min, or log_sqrt.\n",
    "            Defaults to '' \n",
    "        \"\"\"\n",
    "        \n",
    "        if (x, y) == (None, None):\n",
    "            raise ValueError('At least one of the dimensions (x, y) must be specified')\n",
    "        if x == None or y == None:\n",
    "            # If a dataset was given through **kwargs, select missing dimension with PCA\n",
    "            # The ratio of the (x, y) sizes will comply roughly with the ratio of the two largest principal components\n",
    "            if 'data' == None:\n",
    "                raise ValueError(\n",
    "                    \"If one of the dimensions is not specified, a dataset must be provided for automatic size \"\n",
    "                    \"initialization.\")\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "\n",
    "        # Update missing size variable\n",
    "        if x == None:\n",
    "            x = y // ratio\n",
    "        if y == None:\n",
    "            y = x // ratio\n",
    "            \n",
    "        # Initializing private variables\n",
    "        \n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self._shape = (np.uint(x), np.uint(y))\n",
    "        self._input_len = np.uint(input_len)\n",
    "        self._learning_rate = float(learning_rate)\n",
    "        self._learning_rate_decay = learning_rate_decay\n",
    "        self._neighborhood_radius = float(neighborhood_radius)\n",
    "        self._neighborhood_radius_decay = neighborhood_radius_decay\n",
    "        self._neighborhood_function = {\n",
    "            'gaussian': self._gaussian}[neighborhood_function]\n",
    "        \n",
    "        # string specifying distance function\n",
    "        self.distance_function = distance_function \n",
    "        # distance function itself\n",
    "        self._distance_function = {\n",
    "            'euclidean': _euclidean_distance,\n",
    "            'custom': self.custom_distance,\n",
    "            'cosine': _cosine_distance,\n",
    "            'manhattan': _manhattan_distance,\n",
    "            'chebyshev': _chebyshev_distance\n",
    "        }[distance_function]\n",
    "        \n",
    "        if distance_function == 'custom' and rank_distance == '':\n",
    "            raise ValueError(\"ERROR: rank distance function has not been selected despite custom distance function having been selected\") \n",
    "        if distance_function == 'custom' and rank_distance not in ['euclidean', 'fractional_dif_max', 'fractional_dif_min', 'log_sqrt']:\n",
    "            raise ValueError(\"ERROR: unavailable rnak distance function selected\") \n",
    "        elif distance_function == 'custom': \n",
    "            self.rank_distance = rank_distance\n",
    "            self.node_ranks = np.arange(1, self.x * self.y + 1).reshape(self.x, self.y) # matrix of SOM node ranks\n",
    "        \n",
    "        self._neigx, self._neigy = np.arange(self._shape[0]), np.arange(self._shape[1])\n",
    "        \n",
    "        self.data = data\n",
    "        self.query_scores = query_scores\n",
    "        self.num_images = num_images\n",
    "        \n",
    "        # empty dataframe with column names\n",
    "        self.selected_images = pd.DataFrame(columns = ['filename', 'ID', 'BMU_x', 'BMU_y'])\n",
    "        self.selected_vectors = np.zeros((self.x, self.y, 128))\n",
    "        \n",
    "        # Seed numpy random generator\n",
    "        if random_seed == None:\n",
    "            self._random_seed = np.random.randint(np.random.randint(np.iinfo(np.int32).max))\n",
    "        else:\n",
    "            self._random_seed = int(random_seed)\n",
    "        np.random.seed(self._random_seed)\n",
    "\n",
    "        ## using top N images only! ##\n",
    "        if num_images != 20000 and mode != 'RGB':\n",
    "            \n",
    "            ids = pd.DataFrame(frame_ids, columns=['ID'])\n",
    "            top_ids = ids[0:num_images]\n",
    "            self.query_scores = query_scores[0:num_images]\n",
    "            # fetch only top N images from dataset\n",
    "            self.data = data[top_ids.ID]\n",
    "        \n",
    "        # Random weight initialization\n",
    "        self._weights = np.random.standard_normal(size=(self._shape[0], self._shape[1], self._input_len))\n",
    "        \n",
    "        # adding extra dimension to the data\n",
    "        if input_len == 4 or input_len == 129: \n",
    "             \n",
    "            # adding query score column to data as 129th column\n",
    "            self.data = np.column_stack((self.data, self.query_scores))\n",
    "            \n",
    "            # dividing new column by standard deviation to adjust its variance to 1\n",
    "            self.data[:, input_len-1] /= math.sqrt(self.data[:, input_len-1].var())\n",
    "            \n",
    "            # adding query scores to SOM nodes' 129th dimension as well, considering the position of the nodes\n",
    "            k = 0\n",
    "            for i in range(self.x):\n",
    "                for j in range(self.y):\n",
    "                    self._weights[i, j, input_len-1] = self.data[k, input_len-1]\n",
    "                    k += 1\n",
    "\n",
    "    # remove the extra dimension added to the data \n",
    "    def reset_data(self):\n",
    "\n",
    "        self.data = np.delete(self.data, -1, axis=1)\n",
    "    \n",
    "    ### custom distance function for adding bias based on ranks of node and image ###\n",
    "    def custom_distance(self, data_row, weights):\n",
    "        # custom distance L_new(node, image) = L2(node, image) + X where\n",
    "        # X = 1/i * dist(rank_node, rank_image)\n",
    "        # where dist = Euclidean, Fractional difference, or log(sqrt(rank_node - rank_image))\n",
    "        \n",
    "        # initial distances between data and each weight vector \n",
    "        l2 = np.linalg.norm(np.subtract(data_row, weights), ord=2, axis=-1)\n",
    "\n",
    "        # given an input vector x, compare its 129th dim with dataset (X) and find its rank. \n",
    "        # to find the rank of the current input x, simply calculate the index of the current row vector\n",
    "        # because query scores are sorted in 129th dim of X.\n",
    "        image_rank = np.where((self.data==data_row).all(axis=1))[0][0] + 1 # adding 1 because index starts at 0 and we don't want to divide by 0\n",
    "\n",
    "        ##########\n",
    "        if self.rank_distance == 'euclidean':\n",
    "            dist = np.absolute(np.subtract(image_rank, self.node_ranks)) # euclidean\n",
    "        if self.rank_distance == 'fractional_dif_max':\n",
    "            dist = frac_max(image_rank, self.node_ranks)\n",
    "        if self.rank_distance == 'fractional_dif_min':\n",
    "            dist = frac_min(image_rank, self.node_ranks)\n",
    "        if self.rank_distance == 'log_sqrt':\n",
    "            dist = log_sqrt(image_rank, self.node_ranks)\n",
    "\n",
    "        node_weight = np.arange(1, self.x * self.y + 1).reshape(self.x, self.y)\n",
    "        # commenting this out for now\n",
    "        # node_weight = np.reciprocal(node_weight, dtype=np.float64) \n",
    "\n",
    "        result = l2 + node_weight * dist\n",
    "\n",
    "        return result;\n",
    "        \n",
    "    def activate(self, x: Union[np.ndarray, pd.DataFrame, list]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates distances between an instance x and the weights of the network.\n",
    "        :param x: array-like: Instance to be compared with the weights of the network.\n",
    "        :return: np.ndarray: Distances between x and each weight of the network.\n",
    "        \"\"\"\n",
    "        return self._distance_function(x, self._weights)\n",
    "\n",
    "    def winner(self, x: Union[np.ndarray, pd.DataFrame, list]) -> Union[Iterable, Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Calculates the best-matching unit of the network for an instance x\n",
    "        :param x: array-like: Instance to be compared with the weights of the network.\n",
    "        :return: (int, int): Index of the best-matching unit of x.\n",
    "        \"\"\"\n",
    "        activation_map = self.activate(x)\n",
    "        return np.unravel_index(activation_map.argmin(), activation_map.shape)\n",
    "\n",
    "    def quantization(self, data: Union[np.ndarray, pd.DataFrame, list]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates distances from each instance of 'data' to each of the weights of the network.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: np.ndarray: array of lists of distances from each instance of the dataset\n",
    "            to each weight of the network.\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "        # Quantization - we wanna perform with Euclidean distance regardless of the distance function selected!\n",
    "        return np.array([(_euclidean_distance(i, self._weights[self.winner(i)])) for i in data_array])\n",
    "\n",
    "    def quantization_error(self, data: Union[np.ndarray, pd.DataFrame, list]) -> float:\n",
    "        \"\"\"\n",
    "        Calculates average distance of the weights of the network to their assigned instances from data.\n",
    "        This error is a quality measure for the training process.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: float: Quantization error.\n",
    "        \"\"\"\n",
    "        quantization = self.quantization(data)\n",
    "        return quantization.mean()\n",
    "\n",
    "    def activation_matrix(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the activation matrix of the network for a dataset, i.e., for each node, the count of instances that\n",
    "        have been assigned to it, in the current state.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: np.ndarray: Activation matrix.\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(self.data, pd.DataFrame):\n",
    "            data_array = self.data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(self.data)\n",
    "\n",
    "        activation_matrix = np.zeros(self._shape)\n",
    "        for i in data_array:\n",
    "            activation_matrix[self.winner(i)] += 1\n",
    "        return activation_matrix\n",
    "    \n",
    "    def winner_map(self, data: Union[np.ndarray, pd.DataFrame, list]) -> dict:\n",
    "        \"\"\"\n",
    "        Calculates, for each node (i, j) of the network,\n",
    "        the list of all instances from 'data' that has been assigned to it.\n",
    "        :param data: array-like: Dataset to be compared with the weights of the network.\n",
    "        :return: dict: Winner map.\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "        winner_map = {(i, j): [] for i in range(self._shape[0]) for j in range(self._shape[1])}\n",
    "        for i in data_array:\n",
    "            winner_map[self.winner(i)].append(i)\n",
    "        return winner_map\n",
    "\n",
    "    def train(self, n_iteration: Union[int, None] = None,\n",
    "              mode: str = 'sequential', verbose: bool = False) -> float:\n",
    "        \"\"\"\n",
    "        Trains the self-organizing map, with the dataset 'data', and a certain number of iterations.\n",
    "        :param data: array-like: Dataset for training.\n",
    "        :param n_iteration: int or None: Number of iterations of training.\n",
    "            If None, defaults to 1000 * len(data) for stepwise training modes,\n",
    "            or 10 * len(data) for batch training mode.\n",
    "        :param mode: str: Training mode name. May be either 'random', 'sequential', or 'batch'.\n",
    "            For 'batch' mode, a much smaller number of iterations is needed, but a higher computation power is required\n",
    "            for each individual iteration.\n",
    "        :param verbose: bool: Activate to print useful information to the terminal/console, e.g.,\n",
    "            the progress of the training process\n",
    "        :return: float: Quantization error after training\n",
    "        \"\"\"\n",
    "        # Convert data to numpy array for training\n",
    "        if isinstance(self.data, pd.DataFrame):\n",
    "            data_array = self.data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(self.data)\n",
    "\n",
    "        # If no number of iterations is given, select automatically\n",
    "        if n_iteration == None:\n",
    "            n_iteration = {'random': 1000, 'sequential': 1000, 'batch': 10}[mode] * len(data_array)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Training with\", n_iteration,\n",
    "                  \"iterations.\\nTraining mode:\", mode, sep=' ')\n",
    "\n",
    "        if mode == 'sequential':\n",
    "            \n",
    "            # shuffling indexes\n",
    "            np.random.seed(self._random_seed)\n",
    "            indexes = np.arange(self.num_images)\n",
    "            np.random.shuffle(indexes)\n",
    "        \n",
    "            # Sequential sampling from training dataset\n",
    "            i = 0\n",
    "            while i < n_iteration:\n",
    "                \n",
    "                if (self._input_len == 129 or self._input_len == 4) and i < n_iteration / 2:\n",
    "                    \n",
    "                    # rebiasing before each epoch - force-assign top scores to weights from top row, and vice versa\n",
    "                    sorted_score = np.sort(self.data[:,self._input_len-1])[::-1]\n",
    "                    # top row\n",
    "                    for j in range(self.y):\n",
    "                        self._weights[0, j, self._input_len-1] = sorted_score[j]\n",
    "                    for k in range(self.y):\n",
    "                        self._weights[self.x - 1, k, self._input_len-1] = sorted_score[(self.x-1) * self.y + k]\n",
    "                        \n",
    "                for idx in indexes:\n",
    "\n",
    "                    x = data_array[idx]\n",
    "\n",
    "                    # Calculating decaying alpha and sigma parameters for updating weights\n",
    "                    alpha = self._learning_rate_decay(self._learning_rate, i, n_iteration)\n",
    "                    sigma = self._neighborhood_radius_decay(\n",
    "                        self._neighborhood_radius, i, n_iteration)\n",
    "\n",
    "                    # Finding winner node (best-matching unit)\n",
    "                    winner = self.winner(x)\n",
    "\n",
    "                    # Updating weights, based on current neighborhood function\n",
    "                    self._weights += alpha * self._neighborhood_function(winner, sigma)[..., None] * (\n",
    "                            x - self._weights)\n",
    "\n",
    "                    # Print progress, if verbose is activated\n",
    "                    if verbose:\n",
    "                        print(\"Iteration:\", i, \"/\", n_iteration, sep=' ', end='\\r', flush=True)\n",
    "\n",
    "                    i += 1\n",
    "\n",
    "            print(\"final alpha: \", alpha)\n",
    "            print(\"final sigma: \", sigma)\n",
    "  \n",
    "        elif mode == 'batch':\n",
    "            # Batch training\n",
    "            for it in range(n_iteration):\n",
    "\n",
    "                # Calculating decaying sigma\n",
    "                sigma = self._neighborhood_radius_decay(\n",
    "                    self._neighborhood_radius, it, n_iteration)\n",
    "\n",
    "                # For each node, create a list of instances associated to it\n",
    "                winner_map = self.winner_map(data_array)\n",
    "\n",
    "                # Calculate the weighted average of all instances in the neighborhood of each node\n",
    "                new_weights = np.zeros(self._weights.shape)\n",
    "                for i in winner_map.keys():\n",
    "                    neig = self._neighborhood_function(i, sigma)\n",
    "                    upper, bottom = np.zeros(self._input_len), 0.0\n",
    "                    for j in winner_map.keys():\n",
    "                        upper += neig[j] * np.sum(winner_map[j], axis=0)\n",
    "                        bottom += neig[j] * len(winner_map[j])\n",
    "\n",
    "                    # Only update if there is any instance associated with the winner node or its neighbors\n",
    "                    if bottom != 0:\n",
    "                        new_weights[i] = upper / bottom\n",
    "                        \n",
    "                # Update all nodes concurrently\n",
    "                self._weights = new_weights\n",
    "\n",
    "                # Print progress, if verbose is activated\n",
    "                if verbose:\n",
    "                    print(\"Iteration:\", it, \"/\", n_iteration, sep=' ', end='\\r', flush=True)\n",
    "        else:\n",
    "            # Invalid training mode value\n",
    "            raise ValueError(\n",
    "                \"Invalid value for 'mode' parameter. Value should be in \" + str(['random', 'sequential', 'batch']))\n",
    "\n",
    "        # Compute quantization error\n",
    "        q_error = self.quantization_error(data_array)\n",
    "        if verbose:\n",
    "            print(\"Quantization error:\", q_error, sep=' ')\n",
    "        return q_error\n",
    "\n",
    "    def weight_initialization(self, mode: str = 'random',\n",
    "                              **kwargs: Union[np.ndarray, pd.DataFrame, list, str, int]) -> None:\n",
    "        \"\"\"\n",
    "        Function for weight initialization of the self-organizing map. Calls other methods for each initialization mode.\n",
    "        :param mode: str: Initialization mode. May be either 'random', 'linear', or 'sample'.\n",
    "            Note: Each initialization method may require multiple additional arguments in kwargs.\n",
    "        :param kwargs:\n",
    "            For 'random' initialization mode, 'sample_mode': str may be provided to determine the sampling mode.\n",
    "            'sample_mode' may be either 'standard_normal' (default) or 'uniform'.\n",
    "            For 'random' and 'sample' modes, 'random_seed': int may be provided for the random value generator.\n",
    "            For 'sample' and 'linear' modes, 'data': array-like must be provided for sampling/PCA.\n",
    "        \"\"\"\n",
    "        modes = {'random': self._weight_initialization_random,\n",
    "                 'sample': self._weight_initialization_sample}\n",
    "        try:\n",
    "            modes[mode](**kwargs)\n",
    "        except KeyError:\n",
    "            raise ValueError(\"Invalid value for 'mode' parameter. Value should be in \" + str(modes.keys()))\n",
    "\n",
    "    def _weight_initialization_random(self, sample_mode: str = 'standard_normal',\n",
    "                                      random_seed: Union[int, None] = None) -> None:\n",
    "        \"\"\"\n",
    "        Random initialization method. Assigns weights from a random distribution defined by 'sample_mode'.\n",
    "        :param sample_mode: str: Distribution for random sampling. May be either 'uniform' or 'standard_normal'.\n",
    "            Defaults to 'standard_normal'.\n",
    "        :param random_seed: int or None: Seed for NumPy random value generator. Defaults to None.\n",
    "        \"\"\"\n",
    "        sample_modes = {'uniform': np.random.random, 'standard_normal': np.random.standard_normal}\n",
    "\n",
    "        # Seed numpy random generator\n",
    "        if random_seed == None:\n",
    "            random_seed = np.random.randint(\n",
    "                np.random.randint(np.iinfo(np.int32).max))\n",
    "        else:\n",
    "            random_seed = int(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        # Initialize weights randomly\n",
    "        try:\n",
    "            self._weights = sample_modes[sample_mode](size=self._weights.shape)\n",
    "        except KeyError:\n",
    "            raise ValueError(\n",
    "                \"Invalid value for 'sample_mode' parameter. Value should be in \" + str(sample_modes.keys()))\n",
    "\n",
    "    def _weight_initialization_sample(self, data: Union[np.ndarray, pd.DataFrame, list],\n",
    "                                      random_seed: Union[int, None] = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialization method. Assigns weights to random samples from an input dataset.\n",
    "        :param data: Dataset for weight initialization/sampling.\n",
    "        :param random_seed: int or None: Seed for NumPy random value generator. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Seed numpy random generator\n",
    "        if random_seed == None:\n",
    "            random_seed = np.random.randint(\n",
    "                np.random.randint(np.iinfo(np.int32).max))\n",
    "        else:\n",
    "            random_seed = int(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        # Convert data to numpy array for training\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            data_array = data.to_numpy()\n",
    "        else:\n",
    "            data_array = np.array(data)\n",
    "\n",
    "        # Assign weights to random samples from dataset\n",
    "        sample_size = self._shape[0] * self._shape[1]\n",
    "        sample = np.random.choice(len(data_array), size=sample_size,\n",
    "                                  replace=(sample_size > len(data_array)))\n",
    "        self._weights = data_array[sample].reshape(self._weights.shape)\n",
    "\n",
    "    def _gaussian(self, c: Tuple[int, int], sigma: float) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Gaussian neighborhood function, centered in c. \n",
    "        :param c: (int, int): Center coordinates for gaussian function.\n",
    "        :param sigma: float: Spread variable for gaussian function.\n",
    "        :return: np.ndarray: Gaussian, centered in c, over all the weights of the network.\n",
    "        \"\"\"\n",
    "        # Calculate coefficient with sigma\n",
    "        d = 2 * sigma * sigma\n",
    "        # Calculate vertical and horizontal distances\n",
    "        dx = self._neigx - c[0]\n",
    "        dy = self._neigy - c[1]\n",
    "\n",
    "        # Calculate gaussian centered in c\n",
    "        ax = np.exp(-np.power(dx, 2) / d)\n",
    "        ay = np.exp(-np.power(dy, 2) / d)\n",
    "        return np.outer(ax, ay)\n",
    "    \n",
    "    \n",
    "    ## custom functions for displaying images ##\n",
    "    \n",
    "\n",
    "    \n",
    "    ############################\n",
    "    \n",
    "    def display_results(self, frameID_scores, original_data, ranking=False):\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        # putting the 3 functions below together\n",
    "        filepaths = pd.read_csv(r\"native-queries\\frame-ID-to-filepath.csv\",sep=' ',names=['filename', 'ID'])\n",
    "        \n",
    "        if self.num_images < 20000:\n",
    "            frameID_scores = frameID_scores.head(self.num_images)\n",
    "        \n",
    "        if self.num_images == (self.x * self.y):\n",
    "            self.hungarian_algo(filepaths, frameID_scores)\n",
    "        \n",
    "        else:\n",
    "            df = self.node2image(filepaths)\n",
    "            self.select_images(df, frameID_scores)\n",
    "        \n",
    "        self.display_images(original_data)\n",
    "        \n",
    "        if ranking == True:\n",
    "            self.add_ranking()\n",
    "\n",
    "        print(\"Time elapsed: %s seconds\" % (time() - start_time))\n",
    "    \n",
    "    def hungarian_algo(self, filepaths, frameID_scores):\n",
    "        \n",
    "        # frameID_scores = frameID_scores[0:self.num_images]\n",
    "        # self.selected_images = pd.DataFrame(columns = ['filename', 'ID', 'BMU_x', 'BMU_y'])\n",
    "        \n",
    "        # merge with filepaths on top 100 IDs only \n",
    "        merged = frameID_scores.merge(filepaths, how='inner', on='ID') \n",
    "        \n",
    "        # pairwise distance matrix for SOM weight vectors and input data \n",
    "        pairwise_dist = distance_matrix(self.data, np.concatenate(self._weights)) #swapped back to normal again\n",
    "        row, column = linear_sum_assignment(pairwise_dist)\n",
    "\n",
    "        my_pd = pd.DataFrame(columns = ['ID', 'BMU_x', 'BMU_y'])\n",
    "\n",
    "        for i in range(self.num_images):\n",
    "            \n",
    "            # BMU indexes \n",
    "            x = column[i] / self.y\n",
    "            y = column[i] % self.y\n",
    "            \n",
    "            my_pd = my_pd.append({'ID': merged.ID[row[i]],'BMU_x': int(x), 'BMU_y': y}, ignore_index=True)\n",
    "        \n",
    "        merged = my_pd.merge(merged, on='ID')\n",
    "        \n",
    "        print(\"The following images were selected by Hungarian algo: \")\n",
    "        sorted_images = merged.sort_values(by=['BMU_x', 'BMU_y'])\n",
    "        display(sorted_images)\n",
    "        \n",
    "        self.selected_images = sorted_images\n",
    "    \n",
    "    def node2image(self, filepaths):\n",
    "\n",
    "        # run find_bmu to obtain BMU for each feature vector\n",
    "        bmu_list = []\n",
    "        for vec in self.data:\n",
    "            bmu_list.append(np.asarray(self.winner(vec)))\n",
    "\n",
    "        # storing the result in a dataframe. \n",
    "        # i-th row of the dataframe (ex: 4 1) is associated with the BMU coordinates for the i-th feature vector  \n",
    "        data_ = pd.DataFrame(bmu_list, columns=['BMU_x','BMU_y'])\n",
    "\n",
    "        # merge the 2 dataframes\n",
    "        df = pd.concat([filepaths, data_], axis=1, join='inner')\n",
    "\n",
    "        # now we have:\n",
    "\n",
    "        # filename ID BMU_x BMU_y\n",
    "        # ........ rows ........\n",
    "\n",
    "        return df\n",
    "\n",
    "    # image selection after training\n",
    "    def select_images(self, df, id_scores):\n",
    "\n",
    "        # empty dataframe with column names\n",
    "        ##selected_images = self.selected_images\n",
    "        selected_images = pd.DataFrame(columns = ['filename', 'ID', 'BMU_x', 'BMU_y'])\n",
    "        \n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "\n",
    "                tmp = df.loc[(df.BMU_x == i) & (df.BMU_y == j)]\n",
    "                display(tmp)\n",
    "                if(len(tmp) != 0):\n",
    "                    \n",
    "                    merged = tmp.merge(id_scores, how='inner', on='ID') \n",
    "                    \n",
    "                    # select image with highest score\n",
    "                    highest = merged.loc[merged['query_scores'].idxmax()]\n",
    "                    \n",
    "                    selected_images = selected_images.append(highest)\n",
    "\n",
    "        print(\"The following images were selected: \")\n",
    "        display(selected_images)\n",
    "\n",
    "        self.selected_images = selected_images\n",
    "    \n",
    "    def display_images(self, original_data):\n",
    "\n",
    "        # this function not only displays images, but also removes the extra dimension we added to the data\n",
    "        # and extracts selected vectors for each SOM node\n",
    "        \n",
    "        my_dir = \"native-queries/thumbs/\"\n",
    "       \n",
    "        fig, ax = plt.subplots(self.x, self.y, sharex='col', sharey='row', figsize=(32 * self.y, 18  * self.x))\n",
    "        \n",
    "        # remove the extra dimension we initially added to the data\n",
    "        if self._input_len == 129:\n",
    "            self.reset_data()\n",
    "        for i in range(len(self.selected_images)):\n",
    "\n",
    "            row = self.selected_images.iloc[i,:]\n",
    "            x = row.BMU_x\n",
    "            y = row.BMU_y\n",
    "            id = row.ID\n",
    "        \n",
    "            # extracting selected vectors used for output - for evaluation purposes later\n",
    "            self.selected_vectors[x, y] = original_data[id]\n",
    "\n",
    "            img = Image.open(my_dir + row.filename)\n",
    "            img = img.resize((320, 180))\n",
    "            img = np.asarray(img)\n",
    "\n",
    "            ax[x, y].imshow(img, aspect='auto')\n",
    "            \n",
    "            # add a patch to visualize the original ranking of the associated image\n",
    "            # top image from original ranking is colored mint, and others shades of green depending on their query scores\n",
    "            if row.query_scores == self.selected_images['query_scores'].max():\n",
    "                rect = patches.Rectangle((275, 150), 40, 25, color=(0.07, 0.95, 0.9), fill=True)\n",
    "                \n",
    "            else:\n",
    "                rect = patches.Rectangle((275, 150), 40, 25, color=(0, row.query_scores, 0), fill=True)\n",
    "            \n",
    "            ax[x, y].add_patch(rect)\n",
    "        \n",
    "        #plt.tight_layout(pad=0.1, w_pad=0.1, h_pad=0.1)\n",
    "        \n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "        plt.axis('scaled')\n",
    "        plt.show()\n",
    "        \n",
    "   # adding ranking to selected images \n",
    "    def add_ranking(self):\n",
    "        \n",
    "        selected = self.selected_images\n",
    "        \n",
    "        selected['SOM_rank'] = np.arange(len(selected)) + 1\n",
    "        selected['QS_rank'] = selected['query_scores'].rank(ascending=False).astype(int)\n",
    "    \n",
    "        self.selected_images = selected\n",
    "        \n",
    "        print(\"ranking: \")\n",
    "        display(selected)\n",
    " \n",
    "    ## evaluation metrics ## \n",
    "    # comparing the distance of selected images on the output screen and in the feature space\n",
    "    \n",
    "    # here we use 2 different notions of \"neighbor\" - direct neighbors (neighbors in all cardinal directions) and next door neighbors.\n",
    "    def mean_distance(self, print_result=False):\n",
    "        # mean distance matrix[i,j] contains the mean distance of neighbors from selected_vectors[i, j]\n",
    "        mean_distance_matrix = np.zeros((som_mk.x, som_mk.y))\n",
    "        distance_sum = 0\n",
    "        \n",
    "        # hardcoding. Not elegent but well, it works. #\n",
    "        for i in range(som_mk.x):\n",
    "            for j in range(som_mk.y):\n",
    "\n",
    "                distance_sum = 0\n",
    "                #TODO!\n",
    "                \n",
    "                    \n",
    "        if print_result == True:\n",
    "            print(\"\\nmean_distance: \\n\")\n",
    "            print(mean_distance_matrix)\n",
    "\n",
    "        return mean_distance_matrix\n",
    "                \n",
    "    def mean_distance_nextdoor(self, print_result=False):\n",
    "        \n",
    "        mean_distance_matrix = np.zeros((self.x, self.y))\n",
    "        \n",
    "        for i in range(self.x):\n",
    "            for j in range(self.y):\n",
    "                if j == 0:\n",
    "                    mean_distance_matrix[i, j] = _euclidean_distance(self.selected_vectors[i,j+1], self.selected_vectors[i, j])\n",
    "                elif j == self.y-1:\n",
    "                    mean_distance_matrix[i, j] = _euclidean_distance(self.selected_vectors[i, j-1], self.selected_vectors[i, j])\n",
    "                else:\n",
    "                    mean_distance_matrix[i, j] = (_euclidean_distance(self.selected_vectors[i, j-1], self.selected_vectors[i, j]) \n",
    "                    + _euclidean_distance(self.selected_vectors[i, j+1], self.selected_vectors[i, j])) / 2\n",
    "                    \n",
    "        if print_result == True:\n",
    "            print(\"\\nmean_distance_nextdoor: \\n\")\n",
    "            print(mean_distance_matrix)\n",
    "                    \n",
    "        return mean_distance_matrix\n",
    "    \n",
    "    ## Rank correlation metrics ## \n",
    "    # comparing the original ranking with SOM-induced ranking\n",
    "        \n",
    "    def tau(self):\n",
    "        \n",
    "        return self.selected_images['SOM_rank'].corr(self.selected_images['QS_rank'], method='kendall')\n",
    "\n",
    "    def nDCG(self):\n",
    "        ## use original scores, not rankings. ##\n",
    "    \n",
    "        # Releveance scores in actual order\n",
    "        actual = self.selected_images['query_scores']\n",
    "        # in ideal order\n",
    "        ideal = actual.sort_values()\n",
    "        \n",
    "        actual = np.asarray(actual).reshape(1, self.num_images)\n",
    "        ideal = np.asarray(ideal).reshape(1, self.num_images)\n",
    "\n",
    "        return ndcg_score(actual, ideal)\n",
    "    \n",
    "    ### for RGB example ###\n",
    "    def rgb(self, query_scores):\n",
    "        \n",
    "        selected_images = pd.DataFrame(columns = ['BMU_x', 'BMU_y', 'R', 'G', 'B', 'Query_scores'])\n",
    "        \n",
    "        # hungarian algorithm\n",
    "        pairwise_dist = distance_matrix(self.data, np.concatenate(self._weights))\n",
    "        row, column = linear_sum_assignment(pairwise_dist)\n",
    "\n",
    "        for i in range(self.num_images):\n",
    "            \n",
    "            # BMU indexes \n",
    "            x = column[i] / self.y\n",
    "            y = column[i] % self.y\n",
    "            \n",
    "            selected_images = selected_images.append({'BMU_x': int(x), 'BMU_y': y}, ignore_index=True)\n",
    "        \n",
    "        selected_images['BMU_x'] = selected_images['BMU_x'].astype(int)\n",
    "        selected_images['BMU_y'] = selected_images['BMU_y'].astype(int)\n",
    "\n",
    "        sorted_images = selected_images.sort_values(by=['BMU_x', 'BMU_y'])\n",
    "        \n",
    "        for i, row in sorted_images.iterrows():\n",
    "            \n",
    "            index = row.name\n",
    "            \n",
    "            sorted_images.at[i, 'R'] = self.data[index, 0]\n",
    "            sorted_images.at[i, 'G'] = self.data[index, 1]\n",
    "            sorted_images.at[i, 'B'] = self.data[index, 2]\n",
    "            \n",
    "            sorted_images.at[i, 'Query_scores'] = query_scores[index]\n",
    "\n",
    "        print(\"The following images were selected: \")            \n",
    "        display(sorted_images)\n",
    "        \n",
    "        # visualizing selected images \n",
    "        fig, ax = plt.subplots(self.x, self.y, sharex='col', sharey='row', figsize=(10, 10))\n",
    "        \n",
    "        for i, row in sorted_images.iterrows():\n",
    "        \n",
    "            rgb = np.asarray([row['R'], row['G'], row['B']])\n",
    "            \n",
    "            x = int(row.BMU_x)\n",
    "            y = int(row.BMU_y)\n",
    "            \n",
    "            ax[x, y].imshow([[rgb]])\n",
    "            \n",
    "            ax[x, y].set_xticklabels([])\n",
    "            ax[x, y].set_yticklabels([])\n",
    "            \n",
    "        plt.grid(False)\n",
    "        plt.axis('off')\n",
    "        plt.subplots_adjust(wspace=0, hspace=0)\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c3311-4178-4bb4-91a4-6e798461d4a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final alpha:  0.16529198829732722\n",
      "final sigma:  0.33058397659465444\n",
      "The following images were selected: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BMU_x</th>\n",
       "      <th>BMU_y</th>\n",
       "      <th>R</th>\n",
       "      <th>G</th>\n",
       "      <th>B</th>\n",
       "      <th>Query_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.83385</td>\n",
       "      <td>0.31361</td>\n",
       "      <td>0.72745</td>\n",
       "      <td>0.64062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.68846</td>\n",
       "      <td>0.24030</td>\n",
       "      <td>0.96309</td>\n",
       "      <td>0.88708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.59646</td>\n",
       "      <td>0.15476</td>\n",
       "      <td>0.57021</td>\n",
       "      <td>0.54631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.51128</td>\n",
       "      <td>0.43529</td>\n",
       "      <td>0.76054</td>\n",
       "      <td>0.71101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44975</td>\n",
       "      <td>0.41195</td>\n",
       "      <td>0.65249</td>\n",
       "      <td>0.60520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>29</td>\n",
       "      <td>25</td>\n",
       "      <td>0.15410</td>\n",
       "      <td>0.65568</td>\n",
       "      <td>0.01362</td>\n",
       "      <td>-0.13942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>847</th>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>0.95822</td>\n",
       "      <td>0.30393</td>\n",
       "      <td>0.00389</td>\n",
       "      <td>-0.05857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>0.17583</td>\n",
       "      <td>0.87108</td>\n",
       "      <td>0.05886</td>\n",
       "      <td>-0.17974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>0.11018</td>\n",
       "      <td>0.93494</td>\n",
       "      <td>0.01829</td>\n",
       "      <td>-0.26760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>0.90838</td>\n",
       "      <td>0.13715</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>-0.05738</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     BMU_x  BMU_y       R       G       B  Query_scores\n",
       "181      0      0 0.83385 0.31361 0.72745       0.64062\n",
       "835      0      1 0.68846 0.24030 0.96309       0.88708\n",
       "713      0      2 0.59646 0.15476 0.57021       0.54631\n",
       "533      0      3 0.51128 0.43529 0.76054       0.71101\n",
       "218      0      4 0.44975 0.41195 0.65249       0.60520\n",
       "..     ...    ...     ...     ...     ...           ...\n",
       "597     29     25 0.15410 0.65568 0.01362      -0.13942\n",
       "847     29     26 0.95822 0.30393 0.00389      -0.05857\n",
       "638     29     27 0.17583 0.87108 0.05886      -0.17974\n",
       "686     29     28 0.11018 0.93494 0.01829      -0.26760\n",
       "317     29     29 0.90838 0.13715 0.00178      -0.05738\n",
       "\n",
       "[900 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Generating random colors ###\n",
    "\n",
    "np.random.seed(1903)\n",
    "tuple_size = (900, 3)\n",
    "colors = np.random.uniform(0, 1, tuple_size)\n",
    "\n",
    "# target: purple\n",
    "target = np.array([0.6, 0.3, 1.0])\n",
    "plt.imshow([[target]])\n",
    "\n",
    "# calculating scores with respect to the target image - the further an image is from the target (i.e. greater distance) the lower the score.\n",
    "scores = np.zeros((900, 1))\n",
    "for i, x in enumerate(colors):\n",
    "    \n",
    "    scores[i] = 1 - np.linalg.norm(np.subtract(target, x))\n",
    "    \n",
    "#som_RGB = SOM(x=30, y=30, input_len=3, learning_rate=0.5, neighborhood_radius=1.0, neighborhood_function='gaussian', \n",
    "#              data=colors, num_images=900, mode='RGB', query_scores=scores)\n",
    "som_RGB_mk = SOM(x=30, y=30, input_len=4, learning_rate=0.5, neighborhood_radius=1.0, neighborhood_function='gaussian', \n",
    "              data=colors, num_images=900, mode='RGB', query_scores=scores, var_adjustment=True)\n",
    "\n",
    "som_RGB_mk.train(mode='sequential', n_iteration=40000)\n",
    "som_RGB_mk.rgb(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a743db2-57a7-42af-a89e-7187eb4e4ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_RGB_mk = SOM(x=30, y=30, input_len=4, learning_rate=0.5, neighborhood_radius=1.0, neighborhood_function='gaussian', \n",
    "              data=colors, num_images=900, mode='RGB', query_scores=scores, var_adjustment=True)\n",
    "\n",
    "som_RGB.train(mode='sequential', n_iteration=40000)\n",
    "som_RGB.rgb(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8fd7e5-d87c-4c23-8b06-73617d1202d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_LP = SOM(x=20, y=10, input_len=128, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[0], frame_ids=frame_ids[0], distance_function='custom', rank_distance='fractional_dif_max')\n",
    "\n",
    "som_LP.train(mode='sequential', n_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a61994a-4082-4bc0-8956-4d6c4dc29ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[0], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "som_LP.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee283c-94f2-45df-ad99-0b5685612841",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = SOM(x=20, y=10, input_len=129, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[0], frame_ids=frame_ids[0], var_adjustment=True)\n",
    "\n",
    "test.train(mode='sequential', n_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1c01c-82df-415a-b7ed-92e66af2a04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[0], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "test.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28de4f-63f2-4ee4-8339-2f20fb3362c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_som = SOM(x=20, y=10, input_len=128, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[0], frame_ids=frame_ids[0])\n",
    "\n",
    "vanilla_som.train(mode='sequential', n_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd328a12-906f-4829-9e7c-13447c9018a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_som.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb771a3-02b1-4863-a6e7-e610cd53f435",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank_distance = euclidean, fractional_dif_max, fractional_dif_min, log_sqrt\n",
    "som_LP = SOM(x=20, y=10, input_len=128, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[1], frame_ids=frame_ids[1],\n",
    "        distance_function='custom', rank_distance='euclidean')\n",
    "\n",
    "som_LP.train(mode='sequential', n_iteration=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b197c2cd-a239-406f-8584-925e39832f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[1], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[1], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "som_LP.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e579cf53-f530-4888-9e2a-18aa285e0807",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "#    display(som_LP.selected_images)\n",
    "som_LP.selected_images.to_csv('results/som_LP_log-NoReciprocal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76d5058-308c-4d67-9b92-1800c57713ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# regular SOM\n",
    "som = SOM(x=20, y=10, input_len=128, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "        neighborhood_function='gaussian', data=frame_features, num_images=200, random_seed=1903, \n",
    "          query_scores=query_scores[0], frame_ids=frame_ids[0])\n",
    "\n",
    "som.train(mode='sequential', n_iteration=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27867e-796d-4db1-b604-f907a8ce972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[0], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "som.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0975d96-8582-4682-8139-c969a237f1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(som.selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70090f44-382c-436d-9d3d-1249e1b77c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk = SOM(x=20, y=10, input_len=129, learning_rate=0.05, neighborhood_radius=1.0,\n",
    "             neighborhood_function='gaussian', data=frame_features, random_seed=1903, \n",
    "             auto_var_adjustment=True, num_images=200, query_scores=query_scores[0], frame_ids=frame_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916e6041-fc5f-4bb5-8109-ac72308d2b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk.train(mode='sequential', n_iteration=10000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2630d-0407-458a-9ee7-b4393c77f031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "b = pd.DataFrame(query_scores[0], columns=['query_scores'])\n",
    "ab = pd.concat([a, b], axis=1, join='inner')\n",
    "\n",
    "som_mk.display_results(ab, frame_features, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbbf9e2-c1ea-47da-bea5-0461a2be4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = som.mean_distance(print_result=True)\n",
    "d2 = som.mean_distance_nextdoor(print_result=True)\n",
    "\n",
    "print(\"\\n=============\\n\")\n",
    "print(d1.mean(axis=1))\n",
    "print(d2.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf95c8-4267-4a05-b791-bfa71ed0c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "d3 = som_mk.mean_distance(print_result=True)\n",
    "d4 = som_mk.mean_distance_nextdoor(print_result=True);\n",
    "\n",
    "print(\"\\n=============\\n\")\n",
    "print(d3.mean(axis=1))\n",
    "print(d4.mean(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72519834-8660-40df-8c77-48c682c70c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "som.tau()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c33c3-fce0-4c41-b729-b53dedec892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "som.nDCG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225b257-23c6-4f82-b1c4-9f1daf16a203",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk.tau()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2c957e-edaa-4430-86b1-31455e0b9679",
   "metadata": {},
   "outputs": [],
   "source": [
    "som_mk.nDCG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de71d1a-5734-45e2-9298-5074952cadde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "    display(som_mk.selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149d57ce-c360-4765-835f-e4aab1c88c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 200\n",
    "\n",
    "####\n",
    "# fetching top N frames for the very first query\n",
    "ids = pd.DataFrame(frame_ids[0], columns=['ID'])\n",
    "top_ids = ids[0:num_images]\n",
    "data = frame_features[top_ids.ID]\n",
    "\n",
    "qs = query_scores[0]\n",
    "top_qs = qs[0:num_images]\n",
    "data = np.column_stack((data, top_qs))\n",
    "\n",
    "# example input row\n",
    "x = data[49]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aacfa5-8b14-49da-8179-5901999b35df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = row vector, b = weights\n",
    "node_ranks = np.arange(1, 20 * 10 + 1).reshape(20, 10) # 10 x 10 matrix of node ranks\n",
    "image_rank = np.where((data==x).all(axis=1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ad769-440a-4ff5-8e65-ed49c00273ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running SOM on all 327 queries\n",
    "\n",
    "mean_dist = np.zeros(shape=(10,10))\n",
    "mean_distN = np.zeros(shape=(10,10))\n",
    "tau = 0\n",
    "nDCG = 0\n",
    "\n",
    "start_time = time()\n",
    "\n",
    "reps = 100\n",
    "\n",
    "for i in range(0, reps):\n",
    "    biased_som = SOM(10, 10, frame_features[0].shape[0], data=frame_features, sigma=8.0, learning_rate=0.5, random_seed=1903, \n",
    "               query_scores=query_scores[i], frame_ids=frame_ids[i], auto_var_adjustment=True, num_images=100)\n",
    "    \n",
    "    biased_som.train_batch(15)\n",
    "    \n",
    "    a = pd.DataFrame(frame_ids[i], columns=['ID'])\n",
    "    b = pd.DataFrame(query_scores[i], columns=['query_scores'])\n",
    "    ab = pd.concat([a, b], axis=1, join='inner')\n",
    "    \n",
    "    biased_som.display_results(ab, frame_features)\n",
    "    \n",
    "    mean_dist += biased_som.mean_distance()\n",
    "    mean_distN += biased_som.mean_distance_nextdoor()\n",
    "    \n",
    "    tau += biased_som.tau()\n",
    "    nDCG += biased_som.nDCG()\n",
    "    \n",
    "    print(\"i: {}\".format(i))\n",
    "\n",
    "mean_dist /= reps\n",
    "mean_distN /= reps\n",
    "\n",
    "print(\"mean distance between surrounding neighbors: \\n{}\".format(mean_dist))\n",
    "print(\"mean distance between next door neighbors: \\n{}\".format(mean_distN))\n",
    "\n",
    "tau /= reps\n",
    "nDCG /= reps\n",
    "\n",
    "print(\"Kendall's Tau Coefficient: {}\".format(tau))\n",
    "print(\"nDCG: {}\".format(nDCG))\n",
    "\n",
    "print(\"Time elapsed: %s seconds\" % (time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
